---
title: "Projeto Crime Rio"
author: "Gabriela Alves"
date: "14/07/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Criminalidade no Rio de Janeiro
Este estudo tem como objetivo criar um ranking com base em dados sobre a 
criminalidade do Estado do Rio de Janeiro. 
A proposta de criação deste ranking se dá por meio da Análise Fatorial por 
Componentes Principais, uma técnica não supervisionada de machine learning que
utiliza variáveis quantitativas para criar fatores que são responsáveis por 
capturar a variância compartilhada entre as variáveis e estes fatores são
ortogonais entre si.
Esta base de dados contém 53 variáveis quantitativas e pode ser encontrada no 
[link](https://www.ispdados.rj.gov.br:4432/estatistica.html). 


``` {r pacote, echo = FALSE, warning = FALSE,message = FALSE,include=FALSE}

pacotes <- c("tidyverse","knitr","kableExtra","car","rgl","gridExtra",
             "PerformanceAnalytics","reshape2","rayshader","psych","pracma",
             "polynom","rqPen","ggrepel", "plotly","factoextra")

if(sum(as.numeric(!pacotes %in% installed.packages())) != 0){
  instalador <- pacotes[!pacotes %in% installed.packages()]
  for(i in 1:length(instalador)) {
    install.packages(instalador, dependencies = T)
    break()}
  sapply(pacotes, require, character = T)  
} else {
  sapply(pacotes, require, character = T) 
}

````
## Análise fatorial por componentes principais
A análise fatorial é uma técnica de aprendizado de máquina não supervisionada.
É uma técnica multivariada que utiliza apenas variáveis quantitativas/métricas
e essas variáveis tem um coeficiente de correlação alto entre si. 
O objetivo da análise fatorial é a extração de fatores a partir de uma matriz de
correlação gerada  dessas variáveis (e por isso tais variáveis devem ser métricas). 
Os fatores representam o comportamento conjunto das variáveis originais, e pode
ser visto como um agrupamento de variáveis.

O método de extração por componentes principais é o mais utilizado para análise
fatorial, este método extrai os fatores a partir das combinações lineares das
variáveis originais.


Esses fatores são ortogonais entre si, isto é, possui correlação 0 entre si, e 
são capazes de representar o comportamento conjunto das variáveis originais, e
pode ajudar na observação de comportamentos  que antes não eram possíveis de 
serem vistos, e portanto pode-se entender como um agrupamento de variáveis.

A análise fatorial por componentes principais tem três grandes usos:

>   1) Redução da dimensionalidade da base dados por meio do agrupamento de variáveis.
>   2) Confirmação de constructos pré determinados.
>   3) Elaboração de rankings por meio da criação de indicadores.

Sendo a elaboração de rankings uma de suas possibilidades é portanto por este
motivo que será utilizada a análise fatorial por componentes principais nesta 
base de dados, que tem por objetivo a criação de um ranking dos municípios do
Rio de Janeiro por meio de variáveis relacionadas à segurança pública.

# Ponderação arbitrária
Nesse momento faço voz para que não se confunda aqui uma variável quantitativa com 
uma qualitativa que seja expressada de forma numérica, como uma variável binária
que representa evento e não evento de algo, ou uma Escala deLikert. 
Tanto a variável binária de evento quanto a Escala de Likert são variáveis
que expressão categorias, logo não são numéricas, não é possível obter delas 
por exemplo, medidas de tendência central,dispersão ou correlação. 
E uma grande ressalva para a Escala de Likert, ainda que seja expressa em números, 
tal variável tem apenas valor semântico, e valor semântico não se mede, 
qual é a diferença numérica deruim para péssimo? Realizar uma análise fatorial
com tais variáveis resultaria em um problema de **Ponderação Arbitária**, onde 
se estará assumindo pesos numéricos para variáveis quali, onde por exemplo, se
estívéssemos falando de raça/cor, e dizemos que a cor branca é representada por
4 e a preta é 2, estamos dizendo com isso que a cor branca vale 2x mais que a 
preta, e podemos levar preconceito para nossa análise, por isso o cuidado ao
trabalhar com variáveis qualitativas.


# Apresentando a base de dados

Para este estudo os dados selecionados são referentes ao ano de 2020, e para cada
município do Rio de Janeiro se tem dados para cada mês. Para evitar um possível
mês com comportamento atípico, os dados mensais foram agregados e agora representam
apenas os dados de 2020, e cada linha é um muicípio.


Ao todo esta base, após filtragem e agregação, contém 92 observações e 54 variáveis, 
onde apenas a primeira, fmun_cod é qualitativa e representa o nome dos municípios
do Estado do Rio de Janeiro. As restantes variáveis são todas quantitativas e 
representam diferentes ocorrências de referentes à cada tipo de crime. 


```{r dados,eval = TRUE, echo = FALSE}


dados <- read.csv2("BaseMunicipioMensal.csv")
dados <- dados %>%
  filter(ano == 2020) %>%
  select(-fmun_cod,-ano, -mes,-mes_ano, -regiao, -fase)
dados <- aggregate(.~fmun, data = dados, FUN= sum)

dados %>%
  kable() %>%
  kable_styling(bootstrap_options = "striped",
                full_width = T,
                font_size = 12) %>% 
  kable_paper() %>%
  scroll_box(width ="900px", height = "400px")

```
# Análise fatorial por componentes principais
## Correlações
A matéria prima de uma análise  fatorial é uma matriz de correlações, que chamaremos
de rho. Um grande desafio ao se lidar com uma grande quantidade de variáveis 
quantitativas, e neste caso temos pouco mais de 50, é que uma análise bivariada
para cada par de variáveis se torna uma atividade difícil.

No entanto, um dos grandes usos de uma analise fatorial por componentes principais
é a redução da dimencionalidade da base de dados, e por isso essa quantidade de 
variáveis será mantida.

```{r correlacoes, eval = T, echo = F,}
rho <- cor(dados[,2:54])
ggplotly(
rho %>% 
  melt() %>% 
  ggplot() +
  geom_tile(aes(x = Var1, y = Var2, fill = value)) +
  geom_text(aes(x = Var1, y = Var2, label = round(x = value, digits = 3)),
            size = 4) +
  labs(x = NULL,
       y = NULL,
       fill = "Correlações") +
  scale_fill_gradient2(low = "dodgerblue4", 
                       mid = "white", 
                       high = "brown4",
                       midpoint = 0) +
  theme(panel.background = element_rect("white"),
        panel.grid = element_line("grey95"),
        panel.border = element_rect(NA),
        legend.position = "bottom",
        axis.text.x = element_text(angle = 0))
)
```

## Teste de Esfericidade de Bartlet
Para saber se é possível utilizar uma análise fatorial em uma base de dados, é 
necessário realizar antes o Teste de Esfericidades de Bartlet. Este teste consiste
na comparação da matriz de correlações dos nossos dados com uma matriz identidade
onde a diagonal principal é dado por 1 e o resto 0, ou seja, é um teste onde a 
hipótese nula é que as correlações se dão de forma aleatória e a alternativa
é que essas correlações não se dão de forma aleatória.

Se o p-value do teste for menor que o nível de significância rejeitamos a hipótese
nula, logo podemos dizer que as correlações não se dão de forma aleatória e podemos
seguir com a análise fatorial por componentes principais. Nesse caso temos que
o pvalue é 0, logo menor que 0.005, e portanto podemos seguir com a análise 
fatorial.

```{r bartlet, echo = F, eval = T}

cortest.bartlett(R = rho)

```
## Rodando a PCA
O pacote utilizado para rodar essa análise fatorial por componentes principais é
o psych, e ele tem uma peculiaridade, os dados precisam ser padronizados **antes**
de rodar o algoritmo. Em via de regra, a análise fatorial não precisa de padronização
só é padronizado na hora de trabalhar com os fatores, então fica a ressalva deste
pacote.

``` {r echo = F, eval = T,warning = FALSE,message = FALSE,include=FALSE}

dados_std <- dados %>%
  column_to_rownames("fmun") %>%
  scale() %>%
  data.frame
afpca <- prcomp(dados_std)
summary(afpca)
```


### Estudando os fatores

O número de fatores gerados por uma PCA é igual ao número de variáveis utilizadas
para construí-la, no entanto não faria sentido usar a quantitade máxima de fatores
visto que um dos objetivos da PCA é a redução de dimensionalidade da base.


Logo, cada fator vai puxar pra si um pouco do comportamento de cada variável e assim
explicar o comportamento conjunto das variáveis, sendo que determinado fator pode
puxar muito pouco de uma variável, e nada de outras como podemos ver a seguir.

```{r echo =  F, warning=F, message=F}

ggplotly(
  data.frame(afpca$rotation) %>%
    mutate(var= names(dados[2:54])) %>%
    melt(id.vars = "var") %>%
    mutate(var = factor(var)) %>%
    ggplot(aes(x=var, y = value, fill = var)) +
    geom_bar(stat = "identity", color = "black")+
    facet_wrap(~variable) +
    labs(x = NULL, y = NULL, fill = "Legenda:") +
    scale_fill_viridis_d()+
    theme_bw() +
    theme(axis.text.x = element_text(angle=90))
 )


```
A partir da tabela a baixo podemos ver todos os fatores gerados pela análise fatorial,
e com isso podemos ver o quanto cada fator consegue explicar do conjunto de 
variáveis. Logo no primeiro fator temos 93% da variância explicada, e somente 
somando todas as variâncias de cada fator teríamos 100% da variância explicada.



```{r echo = F,warning = FALSE,message = FALSE}



data.frame(eigen_value = afpca$sdev ^ 2,
                        variancia_compartilhada = summary(afpca)$importance[2,],
                        variancia_cumulativa = summary(afpca)$importance[3,]) %>% kable() %>%
kable_styling(bootstrap_options = "striped",
full_width = T,
font_size =12) %>%
kable_paper() %>%
scroll_box(width ="900px", height = "400px")
```

### Critério de decisão de fatores
Para a decisão do número de fatores o critério mais utilizado é o Critério da 
Raíz Latente, que consiste na extração de fatores igual ao número de eigen
values maiores que 1. 

Em nosso caso, utilizando este critério poderíamos extrair 2 fatores. E o 
conjunto desses autovalores conseguem explicar cerca de 95% da variância compartilhada
das  variáveis independetes originais. Onde o primeiro fator sozinho consegue
capturar 92.85% da variância compartilhada e o segundo 2.17%.

É também possível decidir a quantitade de fatores utilizados pelo Scree Plot,
onde a decisão se dá devido a inclinação da curva, quando ela para de fazer grandes
quedas, esse é o ponto de corte. E por esse critério 1 fator parece ser suficiente,
ainda mais se considerarmos que o primeiro fator consegue explicar 92.85%.

```{r echo = F,warning = FALSE,message = FALSE}

ggplotly(
  fviz_eig(X= afpca,
           ggtheme = theme_bw(),
           barcolor = "black",
           barfill = "dodgerblue4",
           linecolor = "darkgoldenrod4")
)

```


# Extraindo as cargas fatoriais e comunalidades
Como comentado acima, pelo critério da raíz latente,  o número de extração de fatores
é 2, ou seja, são 2 fatores que tem eigen values > 1. Sendo assim podemos realizar
a extração de 2 fatores. 
Por meio das cargas fatoriais conseguimos ver a correlação entre cada fator e 
cada variável de origem.
Conseguimos então observar que o primeiro fator consegue se relacionar  fortemente
com várias variáveis, e o segundo se correlaciona com menos intensidade e com menos
variáveis.

``` {r echo = F,warning = FALSE,message = FALSE}

```
Já as comunalidades representam a variância total compartilhada de cada uma dar 
variágeis de origem com todos os fatores extraídos. E assim,conseguimos ver o quanto
os fatores conseguem ser eficiêntes, dado que em grande maioria das variáveis
eles juntos conseguem capturar pelo menos 90% da variância total compartilhada.

``` {r echo = F,warning = FALSE,message = FALSE}

k <- sum((afpca$sdev)>1)
cargas_fatoriais <- afpca$rotation[,1:k] %*% diag(afpca$sdev[1:k])

data.frame(cargas_fatoriais) %>%
  rename(F1=X1,
         F2=X2) %>%
  mutate(comunalidades = rowSums(cargas_fatoriais ^2 )) %>%
  kable() %>%
  kable_styling(bootstrap_options = "striped",
                full_width = T,
                font_size = 12) %>%
  kable_paper() %>%
  scroll_box(width = "900px", heigh = "400px")

```



Visualmente conseguimos ver por meio do gráfico de cargas fatoriais, fatores e 
observações que o primeiro fator está capturando os 92.85% de variância total
compartilhada, enquanto o segundo captura apenas 2.18%, e de modo geral,
as vaiáveis estão tendendo a se relacionar mais fortemente com o fator um.


``` {r echo = F, warning = F, message = F}

data.frame(cargas_fatoriais) %>%
  ggplot(aes(x = X1, y = X2)) +
  geom_point(color = "dodgerblue4") +
  geom_hline(yintercept = 0, color = "darkgoldenrod3", linetype = "dashed") +
  geom_vline(xintercept = 0, color = "darkgoldenrod3", linetype = "dashed") +
  geom_text_repel(label = row.names(cargas_fatoriais)) +
  labs(x = paste("F1", paste0("(",
                              round(summary(afpca)$importance[2,1] * 100,
                                    digits = 2),
                              "%)")),
       y = paste("F2", paste0("(",
                              round(summary(afpca)$importance[2,2] * 100,
                                    digits = 2),
                              "%)"))) +
  theme_bw()
  
  
```

 
De acordo com o Scree plot que nos mostra uma boa nota de corte no primeiro fator,
as cargas fatoriais que mostram a alta correleção do primeiro fator com as variáveis 
de origem e o percentual de captura devariância total compartilhada do primeiro fator
(92.85%).


Dado o forte poder de captura da variância capturada pelo primeiro fator, ainda 
que o segundo fator passe pelo Critério da Raíz Latente de Kaiser, para a 
construção do ranking proposto nesse projeto, usaremos apenas o primeiro fator.

## Construção do Ranking
# Scores fatorias

Para poder criar os fatores, primeiramente precisamos criar os scores fatoriais
que nada mais são do que os eigen vectors divido pela raíz quadrada de cada eigen
value. 
A soma da multiplicação dos scores fatoriais com os valores das variáveis originais 
padronizadas é o que gera os Fatores.
``` {r warning = F, echo = F}

scores_fatoriais <- t(afpca$rotation)/afpca$sdev
colnames(scores_fatoriais) <- colnames(dados_std)

scores_fatoriais %>%
  t() %>%
  data.frame() %>%
  rename(PC1 = 1,
         PC2 = 2) %>%
  select(PC1) %>%
  kable() %>%
  kable_styling(bootstrap_options = "striped",
                full_width = T,
                font_size = 12) %>%
  kable_paper() %>%
  scroll_box(width = "900px", heigh = "400px")
```
Para a criação do ranking foi realizado o espelhamento do fator, visto que a 
concentração das cargas fatoriais estava negativa, fazendo com que se mantida
o ranking ficaria com valores negativos. 
Após a soma dos scores fatoriais que foram multiplicados pelas variáveis originais
padronizadas, foi feita a multiplicação do fator pela variância compartilhada 
para cada observação e assim, foi criada a coluna pontuação, que é o ranking 
de segurança pública, onde quando ele é maior para municípios mais "perigosos"
e menor para municípios melhores. Sendo assim temos o município com maior 
pontuação o Rio de Janeiro, e o menor Macuco.

``` {r echo = F, warning =F}

#Proposta de criação do ranking 
# Assumindo apenas F1 como um indicador


score_d1 <- scores_fatoriais [1,]


# Estabelecendo o ranking
F1 <- t(apply(dados_std, 1, function(x) x * score_d1))

#Na construção de rankings no R, devemos efetuar a multiplicação por -1, 
#visto que os scores fatoriais das observações mais fortes são, por padrão, 
#apresentados acompanhados do sinal de menos.

F1 <- data.frame(F1) %>%
  mutate(fator1 = rowSums(.) * -1)

# Importando as colunas de fatores F1 e F2
dados["Fator1"] <- F1$fator1


#Criando um ranking pela soma ponderada dos fatores por sua variância
#compartilhada:

#Calculando a variância compartilhada

var_compartilhada <- (afpca$sdev ^2 /sum(afpca$sdev  ^2))

dados <- dados %>%
  mutate(pontuacao = Fator1 *var_compartilhada[1])


# Visualizando o ranking final
dados %>%
  arrange(desc(pontuacao)) %>%
  kable() %>%
  kable_styling(bootstrap_options = "striped",
                full_width = T,
                font_size = 12) %>%
  kable_paper() %>%
  scroll_box(width = "900px", heigh = "400px")

```

scree plot e criterio da raiz latente de 
https://www.geeksforgeeks.org/sum-of-rows-based-on-column-value-in-r-dataframe/
https://www.biostars.org/p/303219/
https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html

